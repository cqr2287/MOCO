{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-21T02:02:57.498694Z","iopub.execute_input":"2022-03-21T02:02:57.499018Z","iopub.status.idle":"2022-03-21T02:05:12.719700Z","shell.execute_reply.started":"2022-03-21T02:02:57.498980Z","shell.execute_reply":"2022-03-21T02:05:12.718813Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install -U --no-cache-dir gdown --pre","metadata":{"execution":{"iopub.status.busy":"2022-03-21T02:06:33.904659Z","iopub.execute_input":"2022-03-21T02:06:33.905245Z","iopub.status.idle":"2022-03-21T02:06:58.723135Z","shell.execute_reply.started":"2022-03-21T02:06:33.905203Z","shell.execute_reply":"2022-03-21T02:06:58.722299Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"! gdown --id 1enXTrapvY56RzA-F-ec9NoQ2XKpjPAT3","metadata":{"execution":{"iopub.status.busy":"2022-03-21T02:07:03.189219Z","iopub.execute_input":"2022-03-21T02:07:03.189965Z","iopub.status.idle":"2022-03-21T02:07:10.831422Z","shell.execute_reply.started":"2022-03-21T02:07:03.189925Z","shell.execute_reply":"2022-03-21T02:07:10.830497Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"! unzip cifar10","metadata":{"execution":{"iopub.status.busy":"2022-03-21T02:07:20.954078Z","iopub.execute_input":"2022-03-21T02:07:20.954933Z","iopub.status.idle":"2022-03-21T02:07:24.766950Z","shell.execute_reply.started":"2022-03-21T02:07:20.954887Z","shell.execute_reply":"2022-03-21T02:07:24.766042Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nimport torch.utils.data.distributed\nimport random\nimport numpy as np\nimport itertools\nimport os\nfrom tqdm import tqdm\n\n\nclass MoCo(nn.Module):\n    def __init__(self):\n        super(MoCo, self).__init__()\n        torch.manual_seed(2287)\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.q_backbone = torchvision.models.regnet_y_400mf(pretrained=False)\n        self.k_backbone = torchvision.models.regnet_y_400mf(pretrained=False)\n        self.transform_none = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n\n        self.transform_not_none = transforms.Compose([\n            transforms.RandomHorizontalFlip(0.5),\n            transforms.RandomRotation(degrees=180),\n            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4),\n            transforms.RandomGrayscale(p=0.1),\n            transforms.ToTensor(),\n        ])\n        self.loss = nn.CrossEntropyLoss().to(self.device)\n        self.queue = []\n\n        self.train_loader_not_aug, self.train_loader_aug, self.queue_loader = self.get_train_loader()\n\n        self.queue_loader = itertools.cycle(self.queue_loader)\n\n    def q_forward(self, x):\n        return self.q_backbone(x)\n\n    def k_forward(self, x):\n        return self.k_backbone(x)\n\n    def train(self,\n              t=0.1,\n              lr=1e-4,\n              weight_decay=0,\n              batch_size=64,\n              m=0.99,\n              total_epoch=1000\n              ):\n        '''\n        :param t: temperature for softmax\n        :param lr:\n        :param weight_decay:\n        :param batch_size:\n        :param m: momentum of query\n        :param total_epoch:\n        :return:\n        '''\n        if os.path.exists('q_encoder.pth'):\n            self.q_backbone.load_state_dict(torch.load('q_encoder.pth'))\n        if os.path.exists('k_encoder.pth'):\n            self.k_backbone.load_state_dict(torch.load('k_encoder.pth'))\n        for i in range(total_epoch):\n            loss = self.train_one_epoch(t=t, lr=lr, weight_decay=weight_decay, batch_size=batch_size, m=m,)\n            print(f'epoch {i + 1}, loss = {loss}')\n            torch.save(self.q_backbone.state_dict(), 'q_encoder.pth')\n            torch.save(self.k_backbone.state_dict(), 'k_encoder.pth')\n\n    def train_one_epoch(self,\n                        t=0.1,\n                        lr=1e-3,\n                        weight_decay=0,\n                        batch_size=64,\n                        m=0.99,\n                        ):\n        '''\n        :param t: temperature for softmax\n        :param lr:\n        :param weight_decay:\n        :param batch_size:\n        :param m: momentum of query\n        :return:\n        '''\n        optimizer = torch.optim.AdamW(self.q_backbone.parameters(), lr=lr, weight_decay=weight_decay)\n        cosLR = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=len(self.train_loader_not_aug), eta_min=0)\n        self.initialize_queue()\n        epoch_loss = 0\n        for (x_q, _), (x_k, _) in tqdm(zip(self.train_loader_not_aug, self.train_loader_aug)):\n            optimizer.zero_grad()\n            # x_q x_k both (N, D), D is the dimension of picture (C,H,W)\n\n            q = self.q_forward(x_q.to(self.device))\n            # q, k both (N, C)\n            N, C = q.shape\n            k = self.k_forward(x_k.to(self.device))\n            k = k.detach()\n\n            # Nx1\n            l_pos = torch.bmm(q.view(N, 1, C), k.view(N, C, 1)).squeeze(2)\n            # KxD\n            queue = self.get_queue().detach()\n            # print(queue.shape)\n            # NxK\n            l_neg = torch.mm(q.view(N, C), queue.permute(1, 0))\n\n            # Nx(1+K)\n            # print(l_pos.shape, l_neg.shape)\n            logits = torch.cat([l_pos, l_neg], dim=1, )\n\n            labels = torch.zeros(N, dtype=torch.long, device=self.device)\n            loss = self.loss(logits / t, labels)\n            loss.backward()\n            epoch_loss += loss.item()\n            optimizer.step()\n            #cosLR.step()\n\n            self.momentum_synchronize_backbone(m=m)\n            self.enqueue(1)\n            self.dequeue(1)\n\n        epoch_loss /= len(self.train_loader_not_aug)\n        return epoch_loss\n\n    def get_train_loader(self, batch_size=64, seed=2287):\n        train_set = datasets.CIFAR10('./data', train=True, transform=self.transform_none)\n        torch.manual_seed(seed=seed)\n        g = torch.Generator()\n        train_loader_not_aug = DataLoader(train_set, batch_size=batch_size, shuffle=True, generator=g, )\n\n        train_set = datasets.CIFAR10('./data', train=True, transform=self.transform_not_none)\n        torch.manual_seed(seed=seed)\n        g = torch.Generator()\n        train_loader_aug = DataLoader(train_set, batch_size=batch_size, shuffle=True, generator=g)\n\n        # without seed, so the order of queue loader is not same with those above\n        train_set = datasets.CIFAR10('./data', train=True, transform=self.transform_none)\n        queue_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n\n        return train_loader_not_aug, train_loader_aug, queue_loader\n\n    def get_queue(self):\n        '''\n        :return: a tensor, (queue_length*batch, D)\n        '''\n        # print(self.queue)\n        return torch.cat(self.queue, dim=0)\n\n    def initialize_queue(self, queue_length=5, batch_size=64):\n        '''\n        :param queue_lenth: relative length in terms of batch_size.\n        :param batch_size:\n        :return:\n        '''\n        del self.queue\n        self.queue = []\n        self.enqueue(queue_length)\n\n    def enqueue(self, k, ):\n        '''\n        :param k: relative length in terms of batch_size.\n        :return:\n        '''\n        for i in range(k):\n            x, _ = next(self.queue_loader)\n            x = x.to(self.device)\n            self.queue.append(self.k_backbone(x))\n\n    def dequeue(self, k):\n        '''\n        :param k:\n        :return: relative length in terms of batch_size.\n        '''\n        assert len(self.queue) > k, 'cant dequeue because queue are empty!!!'\n        for i in range(k):\n            self.queue.pop(0)\n\n    def empty_queue(self):\n        self.queue = None\n\n    def momentum_synchronize_backbone(self, m):\n        with torch.no_grad():\n            for i, j in zip(self.q_backbone.parameters(), self.k_backbone.parameters()):\n                j = m * j + (1 - m) * i\n","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:05:01.082064Z","iopub.execute_input":"2022-03-21T03:05:01.082421Z","iopub.status.idle":"2022-03-21T03:05:01.114813Z","shell.execute_reply.started":"2022-03-21T03:05:01.082386Z","shell.execute_reply":"2022-03-21T03:05:01.113945Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\na = MoCo().to(device)\na.train()","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:05:07.861091Z","iopub.execute_input":"2022-03-21T03:05:07.861554Z","iopub.status.idle":"2022-03-21T03:38:39.818966Z","shell.execute_reply.started":"2022-03-21T03:05:07.861515Z","shell.execute_reply":"2022-03-21T03:38:39.814384Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets\nfrom tqdm import tqdm\nfrom torchvision import transforms\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, classes=10):\n        '''\n        :param encoder: 'k_encoder' or 'q_encoder' or None\n        '''\n        super(Classifier, self).__init__()\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.net = torchvision.models.regnet_y_400mf(pretrained=False)\n        if encoder is not None:\n            self.net.load_state_dict(torch.load(encoder + '.pth', map_location=device))\n            #self.net.requires_grad_(False)\n        else:\n            pass\n        self.fc = nn.Sequential(\n            nn.LeakyReLU(),\n            nn.Linear(1000, 200),\n            nn.LeakyReLU(),\n            nn.Linear(200,50),\n            nn.LeakyReLU(),\n            nn.Linear(50, classes),\n        )\n\n\n\n    def forward(self, x):\n        x = self.net(x)\n        return self.fc(x)\n\n\ndef get_train_loader(batch_size=64):\n    train_set = datasets.CIFAR10('./data', train=True, transform=transforms.ToTensor())\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, )\n\n    test_set = datasets.CIFAR10('./data', train=False, transform=transforms.ToTensor())\n    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, )\n    return train_loader, test_loader\n\n\ndef train(batch_size=64, lr=1e-3, total_epoch=100, mode=None):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    train_loader, test_loader = get_train_loader(batch_size)\n    model = Classifier(mode).to(device)\n    criterion = nn.CrossEntropyLoss().to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    train_loss_for_draw = []\n    train_acc_for_draw = []\n    valid_loss_for_draw = []\n    valid_acc_for_draw = []\n    best_acc = 0\n    best_loss = 999\n\n    for epoch in range(total_epoch):\n\n        train_loss = 0\n        train_acc = 0\n        for x, y in tqdm(train_loader):\n            optimizer.zero_grad()\n            x = x.to(device)\n            y = y.to(device)\n\n            # N, 10\n            pre = model(x)\n            loss = criterion(pre, y)\n            train_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n            _, predict = torch.max(pre, dim=1)\n            train_acc += (torch.sum((predict == y)).item() / batch_size)\n\n        train_acc /= len(train_loader)\n        train_loss /= len(train_loader)\n        train_loss_for_draw.append(train_loss)\n        train_acc_for_draw.append(train_acc)\n\n        valid_loss = 0\n        valid_acc = 0\n        for x, y in tqdm(test_loader):\n            optimizer.zero_grad()\n            x = x.to(device)\n            y = y.to(device)\n\n            # N, 10\n            pre = model(x)\n            loss = criterion(pre, y)\n            valid_loss += loss.item()\n            loss.backward()\n            _, predict = torch.max(pre, dim=1)\n            valid_acc += (torch.sum((predict == y)).item() / batch_size)\n\n        valid_acc /= len(test_loader)\n        valid_loss /= len(test_loader)\n        valid_loss_for_draw.append(valid_loss)\n        valid_acc_for_draw.append(valid_acc)\n\n        if valid_acc > best_acc:\n            best_acc=valid_acc\n            torch.save(model.state_dict(),'model.pth')\n\n        if valid_loss<best_loss:\n            best_loss=valid_loss\n\n        print(f'epoch {epoch}, train loss = {train_loss}, train acc = {train_acc}, valid loss = {valid_loss}, valid acc = {valid_acc}')\n\n    return valid_loss_for_draw, valid_acc_for_draw, best_acc, best_loss","metadata":{"execution":{"iopub.status.busy":"2022-03-21T05:34:58.626638Z","iopub.execute_input":"2022-03-21T05:34:58.627058Z","iopub.status.idle":"2022-03-21T05:34:58.649121Z","shell.execute_reply.started":"2022-03-21T05:34:58.627020Z","shell.execute_reply":"2022-03-21T05:34:58.648409Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"none_loss, none_acc,a,b = train(mode = None, total_epoch =50)\nprint(a,b)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T05:35:31.846777Z","iopub.execute_input":"2022-03-21T05:35:31.847364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q_loss, q_acc, a, b = train(mode = 'q_encoder',total_epoch =50)\nprint(a,b)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_loss, k_acc,a,b = train(mode = 'k_encoder', total_epoch =50)\nprint(a,b)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport numpy as np\n\nx = np.arange(10)\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.plot(range(len(k_loss)), np.array(k_loss))\nplt.plot(range(len(q_loss)), np.array(q_loss))\nplt.plot(range(len(none_loss)), np.array(none_loss))\n\nplt.legend(['k_encoder/momentum encoder','q_encoder','none'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport numpy as np\n\nx = np.arange(10)\nplt.xlabel('epoch')\nplt.ylabel('acc')\nplt.plot(range(len(k_acc)), np.array(k_acc))\nplt.plot(range(len(q_acc)), np.array(q_acc))\nplt.plot(range(len(none_acc)), np.array(none_acc))\n\nplt.legend(['k_encoder/momentum encoder','q_encoder', 'none'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}